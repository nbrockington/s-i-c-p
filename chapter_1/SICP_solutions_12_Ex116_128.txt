SICP_solutions_12_Ex116_128.txt

Solutions to Exercise 1.16 to 1.28 from SICP textbook Chapter 1.2. 

******
200405
200408

(Exercise 1.16)

; Design a procedure that evolves an iterative exponentiation process
; that uses successive squaring and uses a logarithmic number of
; steps, as does fast-expt. (Hint: Using the observation that
; (b^(n/2))^2 = (b^2)^(n/2), keep, along with the exponent n and the
; base b, an additional state variable a, and define the state
; transformation in such a way that the product a*b^n is unchanged
; from state to state. At the beginning of the process a is taken to
; be 1, and the answer is given by the value of a at the end of the
; process. In general, the technique of defining an invariant quantity
; that remains unchanged from state to state is a powerful way to
; think about the design of iterative algorithms.)


(define	(expti b n)
   (define (expt-iter b	n a)
      (cond ((= n 0) a)
      	        ((even? n) (expt-iter (square b) (/ n 2) a))
      		    (else (expt-iter b (- n 1) (* b a)))))
   (expt-iter b	    n 1))



(Exercise 1.17)

; The exponentiation algorithms in this section are based on
; performing exponentiation by means of repeated multiplication. In a
; similar way, one can perform integer multiplication by means of
; repeated addition. The following multiplication procedure (in which
; it is assumed that our language can only add, not multiply) is
; analogous to the expt procedure:

; (define (* a b)
;   (if (= b 0)                                          
;       0                           
;       (+ a (* a (- b 1)))))                                       

; This algorithm takes a number of steps that is linear in b. Now
; suppose we include, together with addition, operations double, which
; doubles an integer, and halve, which divides an (even) integer by
; 2. Using these, design a multiplication procedure analogous to
; fast-expt that uses a logarithmic number of steps.

(define	    (mult a   b)
   (cond ((= b 0) 0)
      	  ((even? b) (double (mult a (halve b))))
	   (else (+ a (mult a (- b 1))))))

(define	   (double  n)
   (* 2	   n))

(define	   (halve n)
   (/ n	   2))



(Exercise 1.18)

; Using the results of exercises 1.16 and 1.17, devise a procedure
; that generates an iterative process for multiplying two integers in
; terms of adding, doubling, and halving and uses a logarithmic number
; of steps.

(define (multi a b) 
   (define (mult-iter a b c) 
      (cond ((= b 0) c)
            ((even? b) (mult-iter (double a) (/ b 2) c)) 
            (else (mult-iter a (- b 1) (+ c a))))) 
   (mult-iter a b 0))



(Exercise 1.19)

; There is a clever algorithm for computing the Fibonacci numbers in a
; logarithmic number of steps. Recall the transformation of the state
; variables a and b in the fib-iter process of section 1.2.2: a <- a +
; b and b <- a. Call this transformation T, and observe that applying
; T over and over again n times, starting with 1 and 0, produces the
; pair Fib(n+1) and Fib(n). In other words, the Fibonacci numbers are
; produced by applying T^n, the nth power of the transformation T,
; starting with the pair (1,0). Now consider T to be the special case
; of p = 0 and q = 1 in a family of transformations T_pq, where T_pq
; transforms the pair (a,b) according to a <- bq + aq + ap and b <- bp
; + aq. Show that if we apply such a transformation T_pq twice, the
; effect is the same as using a single transformation T_p'q' of the
; same form, and compute p' and q' in terms of p and q. This gives us
; an explicit way to square these transformations, and thus we can
; compute Tn using successive squaring, as in the fast-expt
; procedure. Put this all together to complete the following
; procedure, which runs in a logarithmic number of steps


When (T_pq)^2 is applied to pair (0,1):

a <- (bp + aq)q + (bq + aq + ap)q + (bq + aq + ap)p
   = bpq + aqq + bqq + aqq + apq + bqp + aqp + app
   = b(pq + qq + qp) + a(pq + qq + qp) + a(qq + pp)
   = bq' + aq' + ap'

where p' = pp + qq, q' = 2pq + qq. Similarly,

b <- (bp + qp)p + (bq + aq + ap)q
   = bpp + aqp + bqq + aqq + apq
   = b(qq + pp) + a(pq + qq + qp)
   = bp' + aq'

with p' and q' as above. Hence the iterative Fibonacci procedure, that
runs in a log number of steps, is:


(define (fib n)
  (fib-iter 1 0 0 1 n))

(define (fib-iter a b p q count)
  (cond ((= count 0) b)
  ((even? count)
         (fib-iter a
                   b
                   (+ (* p p) (* q q))                ; compute p'
                   (+ (* 2 p q) (* q q))              ; compute q'
                   (/ count 2)))
		   (else (fib-iter (+ (* b q) (* a q) (* a p))
		   	 	   (+ (* b p) (* a q))
                                   p
                                   q
                                   (- count 1))))))


(Exercise 1.20)

The process that a procedure generates is of course dependent on the
rules used by the interpreter. As an example, consider the iterative
gcd procedure given above. Suppose we were to interpret this procedure
using normal-order evaluation, as discussed in section 1.1.5. (The
normal-order-evaluation rule for if is described in exercise 1.5.)
Using the substitution method (for normal order), illustrate the
process generated in evaluating (gcd 206 40) and indicate the
remainder operations that are actually performed. How many remainder
operations are actually performed in the normal-order evaluation of
(gcd 206 40)? In the applicative-order evaluation?


Using normal-order evaluation, "remainder" is evaluated 18 times:

(gcd 206 40)

(if (= 40 0) [** if-predicate is evaluated, is #f]
    206
    (gcd 40 (remainder 206 40)))
    
(gcd 40 (remainder 206 40))

(if (= (remainder 206 40) 0)    [** x1 evaluation of remainder; #f]
    40
    (gcd (remainder 206 40) (remainder 40 (remainder 206 40))))

(gcd (remainder 206 40) (remainder 40 (remainder 206 40)))

(if (= (remainder 40 (remainder 206 40)) 0)  [** x2 evals of remainder; #f]
   (remainder 206 40)
   (gcd (remainder 40 (remainder 206 40)) 
        (remainder (remainder 206 40) (remainder 40 (remainder 206 40)))))

(gcd (remainder 40 (remainder 206 40))
     (remainder (remainder 206 40) (remainder 40 (remainder 206 40))))

(if (= (remainder (remainder 206 40) 
                  (remainder 40 (remainder 206 40))) 
       0) [** x4 evals; #f]
    (remainder 40 (remainder 206 40))
    (gcd (remainder (remainder 206 40) (remainder 40 (remainder 206 40)))
         (remainder (remainder 40 (remainder 206 40))
                    (remainder (remainder 206 40)
                               (remainder 40 (remainder 206 40))))))

(gcd (remainder (remainder 206 40) (remainder 40 (remainder 206 40)))
     (remainder (remainder 40 (remainder 206 40))
                (remainder (remainder 206 40)
                           (remainder 40 (remainder 206 40)))))

(if (= (remainder (remainder 40 (remainder 206 40))
                  (remainder (remainder 206 40)
                             (remainder 40 (remainder 206 40))))
       0) [** x7 evals; #t]
    (remainder (remainder 206 40) (remainder 40 (remainder 206 40)))
    (gcd... ) [to long to write and will not be evaluated since precicate -> #t]

(remainder (remainder 206 40) (remainder 40 (remainder 206 40))) [** x4 evals]

2. END


Using applicative-order evaluation, remainder is evalauted 4 times:

(gcd 206 40)

(if (= 40 0)
    206
    (gcd 40 (remainder 206 40)))

(gcd 40 (remainder 206 40)) [** x1 eval here]

(gcd 40 6)

(if (= 6 0) 
    40
    (gcd 6 (remainder 40 6)))

(gcd 6 (remainder 40 6)) [** x1 eval here]

(gcd 6 4)

(if (= 4 0)
    6
    (gcd 4 (remainder 6 4)))

(gcd 4 (remainder 6 4)) [** x1 eval here]

(gcd 4 2)

(if (= 2 0)
    4
    (gcd 2 (remainder 4 2)))

(gcd 2 (remainder 4 2)) [** x1 eval here]

(gcd 2 0)

(if (= 0 0)
    2
    (gcd 0 (remainder 2 0)))

2. END


(Exercise 1.21) 

1 ]=> (smallest-divisor 199)

;Value: 199

1 ]=> (smallest-divisor 1999)

;Value: 1999

1 ]=> (smallest-divisor 19999)

;Value: 7



(Exercise 1.22)

; Most Lisp implementations include a primitive called runtime that
; returns an integer that specifies the amount of time the system has
; been running (measured, for example, in microseconds). The following
; timed-prime-test procedure, when called with an integer n, prints n
; and checks to see if n is prime. If n is prime, the procedure prints
; three asterisks followed by the amount of time used in performing
; the test.

; Using this procedure, write a procedure search-for-primes that
; checks the primality of consecutive odd integers in a specified
; range. Use your procedure to find the three smallest primes larger
; than 1000; larger than 10,000; larger than 100,000; larger than
; 1,000,000. Note the time needed to test each prime. Since the
; testing algorithm has order of growth of (n), you should expect that
; testing for primes around 10,000 should take about 10 times as long
; as testing for primes around 1000. Do your timing data bear this
; out? How well do the data for 100,000 and 1,000,000 support the n
; prediction? Is your result compatible with the notion that programs
; on your machine run in time proportional to the number of steps
; required for the computation?

(define (timed-prime-test n)
  (newline)
  (display n)
  (start-prime-test n (runtime)))

(define (start-prime-test n start-time)
  (if (prime? n)
      (report-prime (- (runtime) start-time))))

(define (report-prime elapsed-time)
  (display " *** ")
  (display elapsed-time))


; Search-for-primes procedure: where inputs l and n represent the
; lower bound and number of primes to be found, respectively (i.e.,
; for this questions, n should be 3).

(define (search-for-primes l n)
   (if (even? l)
       (sfp (+ l 1) n)
       (sfp l n)))

(define (sfp l n)
   (cond ((= n 0) '())
         ((prime? l) (sub-sfp l n))
         (else (sfp (+ l 2) n))))

(define (sub-sfp l n)
   (timed-prime-test l)
   (sfp (+ l 2) (- n 1)))


Results of running search-for-primes on lower bounds of increasing
powers of 10:

1 ]=> (search-for-primes (expt 10 4) 3)

10007 *** 0.
10009 *** 0.
10037 *** 0.
;Value: ()

1 ]=> (search-for-primes (expt 10 5) 3)

100003 *** 0.
100019 *** 9.999999999999787e-3
100043 *** 0.
;Value: ()

1 ]=> (search-for-primes (expt 10 6) 3)

1000003 *** 0.
1000033 *** 9.999999999999787e-3
1000037 *** 1.0000000000001563e-2
;Value: ()

1 ]=> (search-for-primes (expt 10 7) 3) 

10000019 *** 9.999999999999787e-3
10000079 *** 9.999999999999787e-3
10000103 *** 9.999999999999787e-3
;Value: ()

1 ]=> (search-for-primes (expt 10 8) 3)

100000007 *** 1.9999999999999574e-2
100000037 *** .03000000000000025
100000039 *** .02999999999999936
;Value: ()

1 ]=> (search-for-primes (expt 10 9) 3)

1000000007 *** .08999999999999986
1000000009 *** .08000000000000007
1000000021 *** .08999999999999986
;Value: ()

1 ]=> (search-for-primes (expt 10 10) 3)

10000000019 *** .27000000000000046
10000000033 *** .2599999999999998
10000000061 *** .2699999999999996
;Value: ()

1 ]=> (search-for-primes (expt 10 11) 3)

100000000003 *** .8300000000000001
100000000019 *** .8300000000000001
100000000057 *** .8100000000000005

1 ]=> (search-for-primes (expt 10 12) 3)

1000000000039 *** 2.6400000000000006
1000000000061 *** 2.5500000000000007
1000000000063 *** 2.5700000000000003
;Value: ()

1 ]=> (search-for-primes (expt 10 13) 3)

10000000000037 *** 8.120000000000005
10000000000051 *** 8.149999999999999
10000000000099 *** 8.100000000000009
;Value: ()

					
Comparison of times for exponents between 9 and 13::

0.085 * 3.16 = 0.269 ≈ mean time for 10^10 (0.267)
0.267 * 3.16 = 0.842 ≈ mean time for 10^11 (0.823)
0.823 * 3.16 = 2.60  ≈ mean time for 10^12 (2.59)
2.59 * 3.16 = 8.18   ≈ mean time for 10^13 (8.12)

These data are compatible with the notion that programs on my machine
run in time proportional to the number of steps required for the
computation, because incrementing the exponent of 10 by 1 (i.e. x10 on
length on input) increase the time taken by approx. sqrt(10).



(Exercise 1.23)

; The smallest-divisor procedure shown at the start of this section
  does lots of needless testing: After it checks to see if the number
  is divisible by 2 there is no point in checking to see if it is
  divisible by any larger even numbers. This suggests that the values
  used for test-divisor should not be 2, 3, 4, 5, 6, ..., but rather
  2, 3, 5, 7, 9, .... To implement this change, define a procedure
  next that returns 3 if its input is equal to 2 and otherwise returns
  its input plus 2. Modify the smallest-divisor procedure to use (next
  test-divisor) instead of (+ test-divisor 1). With timed-prime-test
  incorporating this modified version of smallest-divisor, run the
  test for each of the 12 primes found in exercise 1.22. Since this
  modification halves the number of test steps, you should expect it
  to run about twice as fast. Is this expectation confirmed? If not,
  what is the observed ratio of the speeds of the two algorithms, and
  how do you explain the fact that it is different from 2?


; New smallest-divisor and next procedures: 

(define (next d)
   (if (= d 2)
       3
       (+ d 2)))

; Repeat test for large primes, all other procedures same as for
  Exercise 1.22:

1 ]=> (search-for-primes (expt 10 11) 3)

100000000003 *** .46000000000000796
100000000019 *** .46000000000000796
100000000057 *** .46999999999999886
;Value: ()

1 ]=> (search-for-primes (expt 10 12) 3)

1000000000039 *** 1.4799999999999898
1000000000061 *** 1.4899999999999949
1000000000063 *** 1.4599999999999937
;Value: ()

1 ]=> (search-for-primes (expt 10 13) 3)

10000000000037 *** 4.6299999999999955
10000000000051 *** 4.780000000000001
10000000000099 *** 4.640000000000001
;Value: ()


Ratio of mean times (current test / prev test): 

10^11: 0.463/0.828 = 0.559 
10^12: 1.477/2.59 = 0.570
10^13: 4.683/8.12 = 0.577


The time taken for the amended (search-for-primes) is about 0.57 of
the previous procedure, which is close to but not exactly half. This
is likely to be because although there are half of the number of
divisors to test, an extra procedure (next) is called that was not
called before, and which required an (if) expression to be
evaluated. Furthermore, it could be due to the overhead just due to
calling the (next) procedure.



(Exercise 1.24)

The code for this is obvious. I used n = 100 as the second argument in
(fast-prime?) i.e. number of random numbers to test in Fermat's Little
Theorem.

The results are incredible: 

1 ]=> (search-for-primes (expt 10 13) 3)

10000000000037 *** 3.0000000000000027e-2
10000000000051 *** 3.0000000000000027e-2
10000000000099 *** 3.0000000000000027e-2
;Value: ()

1 ]=> (search-for-primes (expt 10 26) 3)

100000000000000000000000067 *** .06000000000000005
100000000000000000000000123 *** .06000000000000005
100000000000000000000000127 *** .06000000000000005
;Value: ()

1 ]=> (search-for-primes (expt 10 52) 3)

10000000000000000000000000000000000000000000000000327 *** .11999999999999966
10000000000000000000000000000000000000000000000000391 *** .1200000000000001
10000000000000000000000000000000000000000000000000511 *** .1299999999999999
;Value: ()

1 ]=> (search-for-primes (expt 10 104) 3)

100000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000267 *** .2599999999999998
100000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000633 *** .28000000000000025
100000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000721 *** .2599999999999998
;Value: ()


Ratios of mean times: 

10^26 vs. 10^13: 0.06/0.03 = 2.0
10^52 vs. 10^26: 0.123/0.06 = 2.05
10^104 vs. 10^52: 0.267/0.123 = 2.17

Conclusion: Since the Fermat test has O(log(n)) growth, one would
expect that as the size of the input increases by squaring, the time
taken should double. This is indeed what we see in the results:
squaring the size of the input results in approx. doubling of the time
taken to test the primes, even for 100-digit numbers.


(Exercise 1.25)

Alyssa P. Hacker complains that we went to a lot of extra work in
writing expmod. After all, she says, since we already know how to
compute exponentials, we could have simply written

(define (expmod base exp m)
  (remainder (fast-expt base exp) m))

Is she correct? Would this procedure serve as well for our fast prime
tester? Explain.

ANS: Here is the original (exmod):

(define (expmod base exp m)
  (cond ((= exp 0) 1)
        ((even? exp)
         (remainder (square (expmod base (/ exp 2) m))
                    m))
        (else
         (remainder (* base (expmod base (- exp 1) m))
                    m)))) 


And here is Alyssa's version: 

(define (expmod base exp m)
  (remainder (fast-expt base exp) m))

(define (fast-expt b n)
  (cond ((= n 0) 1)
        ((even? n) (square (fast-expt b (/ n 2))))
        (else (* b (fast-expt b (- n 1))))))


Upon evaluation, the difference between the two processes lies in the
fact that a remainder procedure is applied on every recursive call in
the original version of expmod but only applied once, at the end of
the evaluation in Alyssa's version. By the substitution model, the
computation will look something like this for the original expmod:

(remainder (square (remainder (square (... ...) ) ) ) )

And it will look like this for Alyssa's exmod: 

(remainder (square (square (... ...) ) ) )

For large numbers, Alyssa's exmod can grow very quickly with succesive
squaring, potentially causing an integer overflow problem in the
process. On the other hand, the original exmod find the value modulo m
between each application of the exponentiating process, so the carried
variable is scaled back to be smaller than m at each recursive call.



(Exercise 1.26)

Louis Reasoner is having great difficulty doing exercise 1.24. His
fast-prime? test seems to run more slowly than his prime? test. Louis
calls his friend Eva Lu Ator over to help. When they examine Louis's
code, they find that he has rewritten the expmod procedure to use an
explicit multiplication, rather than calling square:

(define (expmod base exp m)
  (cond ((= exp 0) 1)
        ((even? exp)
         (remainder (* (expmod base (/ exp 2) m)
                       (expmod base (/ exp 2) m))
                    m))
        (else
         (remainder (* base (expmod base (- exp 1) m))
                    m))))

``I don't see what difference that could make,'' says Louis. ``I do.''
says Eva. ``By writing the procedure like that, you have transformed
the (log n) process into a (n) process.'' Explain.

ANS. Louis' expmod procedure generates a process whose recursion tree
is binary, that is to say, each recursion calls the original function
twice. This has potentiallly exponential time 2^x, where x is the
number of steps to reach the basis case. However, the increment
applied to the power, n, is to halve it each time, so the basis case
is reached in log(n) time. Hence, the the process has O(2^(log(n)))
time, which is O(n) time.



(Exercise 1.27)

; Demonstrate that the Carmichael numbers listed in footnote 47 really
; do fool the Fermat test. That is, write a procedure that takes an
; integer n and tests whether an is congruent to a modulo n for every
; a<n, and try your procedure on the given Carmichael numbers.

(define (pass-fermat? n)
   (define (check-fermat n base)
      (cond ((= base n) true)
            ((fermat-test n base) (check-fermat n (+ base 1)))
            (else false)))
   (check-fermat n 1))


; Amended (fermat-test) function to accept two arguments:                                      

(define (fermat-test n base)
   (= (expmod base n n) base))


; Provided expmod function:                                                                    

(define (expmod base exp m)
  (cond ((= exp 0) 1)
        ((even? exp)
         (remainder (square (expmod base (/ exp 2) m))
                    m))
        (else
         (remainder (* base (expmod base (- exp 1) m))
                    m))))


Results demonstrating that 103, 561, 1105, 1729, 2465, 2821, 6601 are
Carmichael numbers:

1 ]=> (pass-fermat? 4)

;Value: #f

1 ]=> (pass-fermat? 5)

;Value: #t

1 ]=> (pass-fermat? 100)

;Value: #f

1 ]=> (pass-fermat? 103)

;Value: #t

1 ]=> (pass-fermat? 561)

;Value: #t

1 ]=> (pass-fermat? 1105)

;Value: #t

1 ]=> (pass-fermat? 1729)

;Value: #t

1 ]=> (pass-fermat? 2465)

;Value: #t

1 ]=> (pass-fermat? 2821)

;Value: #t

1 ]=> (pass-fermat? 6601)

;Value: #t



(Exercise 1.28)

;uch a nontrivial square root of 1 exists, then n is not prime. It is
; also possible to prove that if n is an odd number that is not prime,
; then, for at least half the numbers a<n, computing a^(n-1) in this
; way will reveal a nontrivial square root of 1 modulo n. (This is why
; the Miller-Rabin test cannot be fooled.) Modify the expmod procedure
; to signal if it discovers a nontrivial square root of 1, and use
; this to implement the Miller-Rabin test with a procedure analogous
; to fermat-test. Check your procedure by testing various known primes
; and non-primes. Hint: One convenient way to make expmod signal is to
; have it return 0.

; NB. The definition of a M-R witness is wrong in the above paragraph.

(define (miller-rabin-test n)
   (define (try-miller-rabin a)
      (= (expmod-signal a (- n 1) n) 1))
   (try-miller-rabin (+ 1 (random (- n 1)))))


(define (expmod-signal base exp m)
  (cond ((= exp 0) 1)
        ((even? exp)
           (square-mod-with-signal (expmod-signal base (/ exp 2) m) m))
        (else
         (remainder (* base (expmod-signal base (- exp 1) m))
                    m))))


(define (square-mod-with-signal x m)
   (define (signal-nontrivial-sqrt square-x)
      (if (and (= square-x 1)
          (not (= x 1))
          (not (= x (- m 1))))
      0
      square-x))
   (signal-nontrivial-sqrt (remainder (square x) m)))


(define (even? n)
  (= (remainder n 2) 0))


